{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
   "metadata": {},
   "source": [
    "# Understanding Memory in LLMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
   "metadata": {},
   "source": [
    "In the previous Notebook 03, we successfully explored how OpenAI models can enhance the results from Azure Cognitive Search. [Bing Chat](http://chat.bing.com/) is a search engine with a GPT-4 model that utilizes the content of search results to provide context and deliver accurate responses to queries.\n",
    "\n",
    "However, we have yet to discover how to engage in a conversation with the LLM. With Bing Chat, this is possible, as the LLM can understand and reference the previous responses.\n",
    "\n",
    "There is a common misconception that GPT models have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
    "\n",
    "The aim of this Notebook is to demonstrate how we can \"provide memory\" to the LLM by utilizing prompts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d445ce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# .envファイルをロード\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from openai.error import OpenAIError\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import CosmosDBChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from app.utils import (\n",
    "    get_search_results,\n",
    "    order_search_results,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    embed_docs,\n",
    "    search_docs,\n",
    "    get_answer,\n",
    ")\n",
    "\n",
    "from app.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_CHAT_PROMPT\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Don't mess with this unless you really know what you are doing\n",
    "AZURE_SEARCH_API_VERSION = '2021-04-30-Preview'\n",
    "AZURE_OPENAI_API_VERSION = \"2023-03-15-preview\"\n",
    "\n",
    "\n",
    "# Change these below with your own services credentials\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\") #\"Enter your Azure Cognitive Search Endpoint ...\"\n",
    "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\") #\"Enter your Azure Cognitive Search Key ...\"  Make sure is the MANAGEMENT KEY no the query key\n",
    "COG_SERVICES_NAME = os.getenv(\"COG_SERVICES_NAME\") #\"Enter your Cognitive Services NAME, note: not the Endpoint ...\"\n",
    "COG_SERVICES_KEY = os.getenv(\"COG_SERVICES_KEY\") #\"Enter your Cognitive Services Key ...\"\n",
    "AZURE_COSMOSDB_ENDPOINT = os.getenv(\"AZURE_COSMOSDB_ENDPOINT\")#\"ENTER YOUR VALUE\"\n",
    "AZURE_COSMOSDB_NAME = os.getenv(\"AZURE_COSMOSDB_NAME\")#\"ENTER YOUR VALUE\"\n",
    "AZURE_COSMOSDB_CONTAINER_NAME = os.getenv(\"AZURE_COSMOSDB_CONTAINER_NAME\")#\"ENTER YOUR VALUE\"\n",
    "AZURE_COMOSDB_CONNECTION_STRING = os.getenv(\"AZURE_COMOSDB_CONNECTION_STRING\")#\"ENTER YOUR VALUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43f924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cosmosdb-account-airqdqawghv7u.documents.azure.com:443/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AZURE_COSMOSDB_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "# os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "# os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"] = AZURE_OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://aoai-vbd-engine.openai.azure.com/\"#os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"350380392cd8489ca98a44dff79fbe05\"#os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"#os.environ[\"AZURE_OPENAI_API_VERSION\"] = AZURE_OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "\n",
    "os.environ[\"AZURE_SEARCH_KEY\"] = AZURE_SEARCH_KEY\n",
    "os.environ[\"AZURE_SEARCH_ENDPOINT\"] = AZURE_SEARCH_ENDPOINT\n",
    "os.environ[\"AZURE_COSMOSDB_ENDPOINT\"]=  AZURE_COSMOSDB_ENDPOINT\n",
    "os.environ[\"AZURE_COSMOSDB_NAME\"]=  AZURE_COSMOSDB_NAME\n",
    "os.environ[\"AZURE_COSMOSDB_CONTAINER_NAME\"]=  AZURE_COSMOSDB_CONTAINER_NAME\n",
    "os.environ[\"AZURE_COMOSDB_CONNECTION_STRING\"]=  AZURE_COMOSDB_CONNECTION_STRING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
   "metadata": {},
   "source": [
    "### Let's start with the basics\n",
    "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Tell me some use cases for reinforcement learning?\"\n",
    "FOLLOW_UP_QUESTION = \"Can you rephrase what you just said?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Game playing: Reinforcement learning has been used extensively in game playing, such as AlphaGo, AlphaZero, and OpenAI Five, to develop strategies and improve performance.\n",
       "\n",
       "2. Robotics: Reinforcement learning is used to train robots to perform complex tasks, such as grasping objects, navigating through environments, and manipulating objects.\n",
       "\n",
       "3. Autonomous vehicles: Reinforcement learning is used to train autonomous vehicles to make decisions while driving, such as changing lanes, avoiding obstacles, and navigating intersections.\n",
       "\n",
       "4. Personalized recommendations: Reinforcement learning is used to develop personalized recommendations for products and services based on user behavior and preferences.\n",
       "\n",
       "5. Healthcare: Reinforcement learning is used to optimize treatment plans for patients with chronic diseases, such as diabetes and cancer.\n",
       "\n",
       "6. Finance: Reinforcement learning is used to develop trading strategies for financial markets, such as predicting stock prices and managing portfolios.\n",
       "\n",
       "7. Advertising: Reinforcement learning is used to optimize ad placement and targeting to maximize revenue and engagement.\n",
       "\n",
       "8. Energy management: Reinforcement learning is used to optimize energy consumption in buildings and power grids, reducing costs and improving efficiency.\n",
       "\n",
       "9. Education: Reinforcement learning is used to develop personalized learning paths for students based on their performance and progress.\n",
       "\n",
       "10. Agriculture: Reinforcement learning is used to optimize crop yields and reduce waste by predicting weather patterns and soil conditions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "response = chain.run(QUESTION)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize, but as an AI language model, I am unable to determine which statement you are referring to. Can you please provide me with more information or context so that I may assist you better?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "chain.run(FOLLOW_UP_QUESTION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As you can see, it doesn't remember what it just responded. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "                {history}\n",
    "                Human: {question}\n",
    "                AI:\n",
    "            \"\"\"\n",
    "    )\n",
    "chain = LLMChain(llm=llm, prompt=hist_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here are some examples of how reinforcement learning is used in various industries:\\n\\n1. Game playing: Reinforcement learning is used to improve game strategies and performance.\\n2. Robotics: Reinforcement learning is used to train robots to perform complex tasks.\\n3. Autonomous vehicles: Reinforcement learning is used to help autonomous vehicles make driving decisions.\\n4. Personalized recommendations: Reinforcement learning is used to develop personalized product and service recommendations.\\n5. Healthcare: Reinforcement learning is used to optimize treatment plans for patients with chronic diseases.\\n6. Finance: Reinforcement learning is used to develop trading strategies for financial markets.\\n7. Advertising: Reinforcement learning is used to optimize ad placement and targeting.\\n8. Energy management: Reinforcement learning is used to optimize energy consumption in buildings and power grids.\\n9. Education: Reinforcement learning is used to develop personalized learning paths for students.\\n10. Agriculture: Reinforcement learning is used to optimize crop yields and reduce waste.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
   "metadata": {},
   "source": [
    "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
   "metadata": {},
   "source": [
    "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8b27c45-7fbb-40da-a2e3-61e66a8e49b0",
   "metadata": {},
   "source": [
    "In order to not duplicate code, we have put many of the code used in Notebook 3 into functions. These functions are in the app/utils.py and app/prompts.py files This way we can use these functios in the app that we will build later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name = \"cogsrch-index-files\"\n",
    "index2_name = \"cogsrch-index-csv\"\n",
    "indexes = [index1_name, index2_name]\n",
    "\n",
    "agg_search_results = get_search_results(QUESTION, indexes)\n",
    "ordered_results = order_search_results(agg_search_results, reranker_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b2a3595-c3b7-4376-b9c5-0db7a42b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom token limit for gpt-35-turbo : 3000\n",
      "Combined docs tokens count: 58744\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    for page in value[\"chunks\"]:\n",
    "        docs.append(Document(page_content=page, metadata={\"source\": value[\"location\"]}))\n",
    "\n",
    "# Calculate number of tokens of our docs\n",
    "tokens_limit = model_tokens_limit(MODEL)\n",
    "\n",
    "if(len(docs)>0):\n",
    "    num_tokens = num_tokens_from_docs(docs)\n",
    "    print(\"Custom token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Combined docs tokens count:\",num_tokens)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c26d7540-feb8-4581-849e-003f4bf2a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 52\n",
      "Token count after similarity search: 3681\n",
      "Chain Type selected: map_reduce\n",
      "CPU times: user 15.1 s, sys: 2.45 s, total: 17.6 s\n",
      "Wall time: 5.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if num_tokens > tokens_limit:\n",
    "    index = embed_docs(docs)\n",
    "    top_docs = search_docs(index,QUESTION)\n",
    "    \n",
    "    # Now we need to recalculate the tokens count of the top results from similarity vector search\n",
    "    # in order to select the chain type: stuff or map_reduce\n",
    "    \n",
    "    num_tokens = num_tokens_from_docs(top_docs)   \n",
    "    print(\"Token count after similarity search:\", num_tokens)\n",
    "    chain_type = \"map_reduce\" if num_tokens > tokens_limit else \"stuff\"\n",
    "    \n",
    "else:\n",
    "    # if total tokens is less than our limit, we don't need to vectorize and do similarity search\n",
    "    top_docs = docs\n",
    "    chain_type = \"stuff\"\n",
    "    \n",
    "print(\"Chain Type selected:\", chain_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ce6efa9-2b8f-4810-904d-5986b4ae0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some use cases for reinforcement learning include routing, robotics, game playing, autonomous vehicles, elevator scheduling, traffic signal control, and resource management in computer networks. \\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the answer\n",
    "response = get_answer(docs=top_docs, query=QUESTION, language=\"English\", deployment=MODEL, chain_type=chain_type)\n",
    "response['output_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27501f1b-7db0-4ee3-9cb1-e609254ffa3d",
   "metadata": {},
   "source": [
    "And if we ask the follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cf5b323-3b9c-479b-8502-acfc4f7915dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The given content discusses the development of value estimators for reinforcement learning to estimate the value of another policy, resulting in more data-efficient algorithms. The paper also considers bounds on sample size for policy evaluation. There is no relevant text to answer the other questions. \\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_answer(docs=top_docs,  query=FOLLOW_UP_QUESTION, language=\"English\",deployment=MODEL, chain_type=chain_type)\n",
    "response['output_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035fa6e6-226c-400f-a504-30255385f43b",
   "metadata": {},
   "source": [
    "Until now we just have the same as the prior Notebook 03: results from Azure Search enhanced by OpenAI model, with no memory\n",
    "\n",
    "**Now let's add memory to it:**\n",
    "\n",
    "Reference: https://python.langchain.com/en/latest/modules/memory/examples/adding_memory_chain_multiple_inputs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d98b876e-d264-48ae-b5ed-9801d6a9152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reinforcement learning has various use cases, including routing and optimizing long-term return in a class of behaviors. However, the agent does not know the correct behavior or the true model of the environment it interacts with. A policy is often used to choose the action, and the effectiveness of the action is communicated to the agent through a scalar value (reinforcement signal). For more information, you can refer to the following sources: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\")\n",
    "\n",
    "response = get_answer(docs=top_docs, query=QUESTION, language=\"English\", deployment=MODEL, chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "response['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf28927b-d9ee-4412-bb07-13e055e832a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the papers discusses reinforcement learning, which involves finding the optimal course of action in Markovian environments without knowledge of the environment's dynamics. The value of a policy is estimated from results of simulating that policy in the environment, which requires a large amount of simulation as different points in the policy space are considered. Another paper discusses a reinforcement learning algorithm that can learn from scarce experience using a proxy environment. The algorithm relies on four external routines: pick sample, sample, add data, and optimize. The routine pick sample represents the balance between exploration and exploitation and has a single parameter p∗. The larger the value of p∗, the more exploitative the algorithm is. However, there is no relevant text in the given portion of the document to rephrase. For more information, you can refer to the following sources: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf.\\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", deployment=MODEL, chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "response['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3830b0b8-0ca2-4d0a-9747-f6273368002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're welcome! Do you have any other questions I can help you with?\\nSOURCES: N/A\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(docs=top_docs, query=\"Thank you\", language=\"English\", deployment=MODEL, chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "response['output_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111e732b-3c8c-4df3-8fcb-c3d01e7bec74",
   "metadata": {},
   "source": [
    "Let's check our memory to see that it's keeping the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1279692c-7eb0-4300-8a66-c7025f02c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Tell me some use cases for reinforcement learning?\\nAI: Reinforcement learning has various use cases, including routing and optimizing long-term return in a class of behaviors. However, the agent does not know the correct behavior or the true model of the environment it interacts with. A policy is often used to choose the action, and the effectiveness of the action is communicated to the agent through a scalar value (reinforcement signal). For more information, you can refer to the following sources: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf.\\nHuman: Can you rephrase what you just said?\\nAI: One of the papers discusses reinforcement learning, which involves finding the optimal course of action in Markovian environments without knowledge of the environment's dynamics. The value of a policy is estimated from results of simulating that policy in the environment, which requires a large amount of simulation as different points in the policy space are considered. Another paper discusses a reinforcement learning algorithm that can learn from scarce experience using a proxy environment. The algorithm relies on four external routines: pick sample, sample, add data, and optimize. The routine pick sample represents the balance between exploration and exploitation and has a single parameter p∗. The larger the value of p∗, the more exploitative the algorithm is. However, there is no relevant text in the given portion of the document to rephrase. For more information, you can refer to the following sources: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf.\\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf\\nHuman: Thank you\\nAI: You're welcome! Do you have any other questions I can help you with?\\nSOURCES: N/A\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87405173",
   "metadata": {},
   "source": [
    "## Using CosmosDB as persistent memory\n",
    "\n",
    "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wisg to provide recommendations. \n",
    "\n",
    "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
    "We will use a class in LangChain use CosmosDBChatMessageHistory, see [HERE](https://python.langchain.com/en/latest/_modules/langchain/memory/chat_message_histories/cosmos_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7131daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CosmosDB instance from langchain cosmos class.\n",
    "\n",
    "SESSION_ID = '001'  # this isstring value, normally you need to provide the user session id. As an example I have provided '001' as a session id\n",
    "USER_ID = 'dummy01' # this is string value, normally you need to provide the user id. As an example I have provided 'dummy01' as a user id\n",
    "\n",
    "cosmos = CosmosDBChatMessageHistory(\n",
    "    cosmos_endpoint=AZURE_COSMOSDB_ENDPOINT,\n",
    "    cosmos_database=AZURE_COSMOSDB_NAME,\n",
    "    cosmos_container=AZURE_COSMOSDB_CONTAINER_NAME,\n",
    "    connection_string=AZURE_COMOSDB_CONNECTION_STRING,\n",
    "    session_id=SESSION_ID,\n",
    "    user_id=USER_ID\n",
    "    )\n",
    "\n",
    "# prepare the cosmosdb instance\n",
    "cosmos.prepare_cosmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Memory Object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\",chat_memory=cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27ceb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reinforcement learning has been applied successfully to a wide range of problems, including game playing, robotics, control engineering, and telecommunications. It is also increasingly finding use in many important applications, including routing. Research in reinforcement learning focuses on designing algorithms for an agent interacting with an environment, to adjust its behavior in such a way as to optimize a long-term return. This means searching for an optimal behavior in a class of behaviors. Unfortunately, I couldn't find more specific use cases. \\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing using our Question\n",
    "response = get_answer(docs=top_docs, query=QUESTION, language=\"English\", deployment=MODEL, chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "response['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a5ff826",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A single document was longer than the context length, we cannot handle this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Now we add a follow up question:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[39m=\u001b[39m get_answer(docs\u001b[39m=\u001b[39;49mtop_docs, query\u001b[39m=\u001b[39;49mFOLLOW_UP_QUESTION, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEnglish\u001b[39;49m\u001b[39m\"\u001b[39;49m, deployment\u001b[39m=\u001b[39;49mMODEL, chain_type\u001b[39m=\u001b[39;49mchain_type, \n\u001b[1;32m      3\u001b[0m                       memory\u001b[39m=\u001b[39;49mmemory)\n\u001b[1;32m      4\u001b[0m response[\u001b[39m'\u001b[39m\u001b[39moutput_text\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/workspaces/GPT-Azure-Search-Engine/app/utils.py:203\u001b[0m, in \u001b[0;36mget_answer\u001b[0;34m(docs, query, language, deployment, chain_type, memory, temperature, max_tokens)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError: chain_type\u001b[39m\u001b[39m\"\u001b[39m, chain_type, \u001b[39m\"\u001b[39m\u001b[39mnot supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m answer \u001b[39m=\u001b[39m chain( {\u001b[39m\"\u001b[39;49m\u001b[39minput_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs, \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query, \u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m: language}, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    205\u001b[0m \u001b[39mreturn\u001b[39;00m answer\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/chains/combine_documents/map_reduce.py:149\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[39mCombine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mThis reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mapply(\n\u001b[1;32m    145\u001b[0m     \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     [{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_variable_name: d\u001b[39m.\u001b[39mpage_content, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs} \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m docs],\n\u001b[1;32m    147\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_results(\n\u001b[1;32m    150\u001b[0m     results, docs, token_max, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/chains/combine_documents/map_reduce.py:191\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain._process_results\u001b[0;34m(self, results, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collapse_chain\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    187\u001b[0m         input_documents\u001b[39m=\u001b[39mdocs, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mwhile\u001b[39;00m num_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m num_tokens \u001b[39m>\u001b[39m token_max:\n\u001b[0;32m--> 191\u001b[0m     new_result_doc_list \u001b[39m=\u001b[39m _split_list_of_docs(\n\u001b[1;32m    192\u001b[0m         result_docs, length_func, token_max, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     result_docs \u001b[39m=\u001b[39m []\n\u001b[1;32m    195\u001b[0m     \u001b[39mfor\u001b[39;00m docs \u001b[39min\u001b[39;00m new_result_doc_list:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/chains/combine_documents/map_reduce.py:32\u001b[0m, in \u001b[0;36m_split_list_of_docs\u001b[0;34m(docs, length_func, token_max, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m _num_tokens \u001b[39m>\u001b[39m token_max:\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_sub_result_docs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA single document was longer than the context length,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m we cannot handle this.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_sub_result_docs) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     37\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA single document was so long it could not be combined \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith another document, we cannot handle this.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: A single document was longer than the context length, we cannot handle this."
     ]
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", deployment=MODEL, chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "response['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be1620fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're welcome! Let me know if you have any other questions. \\nSOURCES: N/A\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(docs=top_docs, query=\"Thank you\", language=\"English\", deployment=MODEL, chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "response['output_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdc5ac98",
   "metadata": {},
   "source": [
    "Let's check our Azure CosmosDB to see the whole conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1d7688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me some use cases for reinforcement learning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"Reinforcement learning has been applied successfully to a variety of problems, including game playing, automated control of simulated and real robots, autonomous vehicles, and routing. The learning system does not know the correct behavior, or the true model of the environment it interacts with. Given the sensation of the environment state as an input, the agent chooses the action according to some rule, often called a policy. This action constitutes the output. The issue of finding a near-optimal policy from a given class of policies is analogous to a similar issue in supervised learning. There we are looking for a near-optimal hypothesis from a given class of hypotheses. Unfortunately, I couldn't find more specific use cases. \\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Tell me some use cases for reinforcement learning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"Reinforcement learning has been applied successfully to a variety of problems, including game playing, automated control of simulated and real robots, autonomous vehicles, and routing. The learning system does not know the correct behavior, or the true model of the environment it interacts with. Given the sensation of the environment state as an input, the agent chooses the action according to some rule, often called a policy. This action constitutes the output. The issue of finding a near-optimal policy from a given class of policies is analogous to a similar issue in supervised learning. Reinforcement learning is also increasingly finding use in many important applications, including routing. It has also been applied to problems in finance, healthcare, and energy management. Unfortunately, I couldn't find more specific use cases.\\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Tell me some use cases for reinforcement learning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"Reinforcement learning has been applied successfully to a wide range of problems, including game playing, robotics, control engineering, and telecommunications. It is also increasingly finding use in many important applications, including routing. Research in reinforcement learning focuses on designing algorithms for an agent interacting with an environment, to adjust its behavior in such a way as to optimize a long-term return. This means searching for an optimal behavior in a class of behaviors. Unfortunately, I couldn't find more specific use cases. \\nSOURCES: https://demodatasetsp.blob.core.windows.net/arxivcs/0105/0105027v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0207/0207073v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204040v1.pdf, https://demodatasetsp.blob.core.windows.net/arxivcs/0204/0204043v1.pdf\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Thank you', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"You're welcome! Let me know if you have any other questions. \\nSOURCES: N/A\", additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load message from cosmosdb\n",
    "cosmos.load_messages()\n",
    "cosmos.messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
   "metadata": {},
   "source": [
    "![CosmosDB Memory](./images/cosmos-chathistory.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
    "\n",
    "We added persitent memory using CosmosDB.\n",
    "\n",
    "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, it struggles to respond to prompts like: Hello, Thank you, Bye, What's your name, What's the weather and any other task that is not search in the knowledge base.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
    "\n",
    "But, does this solve all the possible scenarios that a virtual assistant will require?  **What about if the answer to the Smart Search Engine is not related to text, but instead requires to look into tabular data?** The next notebook 05 explains and solves the tabular problem and the concept of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9cbd8-612d-4e3b-957c-7c829d84cad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
